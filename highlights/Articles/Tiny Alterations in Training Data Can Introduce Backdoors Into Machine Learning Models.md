# Tiny Alterations in Training Data Can Introduce "Backdoors" Into Machine Learning Models

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article2.74d541386bbf.png)

## Metadata
- Author: [[Cory Doctorow]]
- Full Title: Tiny Alterations in Training Data Can Introduce "Backdoors" Into Machine Learning Models
- Category: #articles
- URL: https://boingboing.net/2019/11/25/backdooring-ai.html

## Highlights
- Training data sets are often ad-hoc in nature; they're so large that it's hard to create version-by-version snapshots, and they're also so prone to mislabeling that researchers are always making changes to them in order to improve their accuracy. All of this suggests that poisoning training data might be easier than it sounds. What's more, many models in production use build on "pretrained" models that are already circulating, so any backdoors inserted into these popular models could propagate to other models derived from them.
